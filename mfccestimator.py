# -*- coding: utf-8 -*-
"""MFCCEstimator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQXD3eOG_0bEWVdMyh9ReY91UPov-MFF
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.insert(0,'/content/drive/My Drive/Capstone/part_2_mfcc_estimation')

#import libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim
from sklearn import preprocessing
from sklearn.utils import class_weight
import os
import sys
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from models import CNNQuantized

class MFCCDNN(nn.Module):
	def __init__(self):
		super().__init__()
		self.ln1 = nn.Linear(256, 228)
		self.ln2 = nn.Linear(228, 200)
		self.ln3 = nn.Linear(200, 164)
		self.ln4 = nn.Linear(164, 128)
		self.ln5 = nn.Linear(128, 64)
		self.ln6 = nn.Linear(64, 32)
		self.ln7 = nn.Linear(32, 20)

	#forward propogation
	def forward(self, x):
		x = F.relu(self.ln1(x))
		x = F.relu(self.ln2(x))
		x = F.relu(self.ln3(x))
		x = F.relu(self.ln4(x))
		x = F.relu(self.ln5(x))
		x = F.relu(self.ln6(x))
		x = F.relu(self.ln7(x))
		return x

directory = '/content/drive/MyDrive/Capstone/part_2_mfcc_estimation/'
net = MFCCDNN()
conv_net = CNNQuantized()

from collections import OrderedDict
new_state_dict = OrderedDict()
state_dict = torch.load(directory+'nn05_quantized.pt', map_location=torch.device('cpu'))
for k, v in state_dict.items():
    name = k[7:] # remove `module.`
    new_state_dict[name] = v
# load params
conv_net.load_state_dict(new_state_dict)

# conv_net.load_state_dict(torch.load(directory+'nn05_quantized.pt', map_location=torch.device('cpu')))

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print('param count DNN:', count_parameters(net))
print('param count CNN:', count_parameters(conv_net))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device = 'cpu'
if torch.cuda.device_count() > 1:
  net = nn.DataParallel(net)
net.to(device)
conv_net.to(device)

X_train = np.load(directory + 'train_audio.npy')
y_train = np.load(directory+'train_mfcc_sf2.npy')

# l1 = ['one']
# l1 = l1 * 3140
# l2 = ['two']
# l2 = l2 * 3111
# train_class = l1 + l2

X_test = np.load(directory+'test_audio.npy')
y_test = np.load(directory+'test_mfcc_sf2.npy')
test_label = np.load(directory+'test_label.npy')

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)



def reshape(data, hop_length=128, clip_size=256):
    dim0 = int(((data.shape[1] * 2) + clip_size) * (data.shape[0] / clip_size))
    new = np.zeros((dim0, clip_size))
    j_max = int((data.shape[1] / hop_length))
    for i in range(data.shape[0]):
        for j in range(j_max + 1):
            indx = int(j+(i*(clip_size/2 - 2)))
            if (j == (j_max)):
                new[indx] = np.zeros((clip_size))
            elif (j == (j_max-1)):
                temp = np.zeros(clip_size)
                temp[0:int(clip_size/2)] = data[i, (j*hop_length):((j*hop_length) + int((clip_size/2)))]
                new[indx] = temp
            else:
                new[indx] = data[i, (j*hop_length):(j*hop_length+clip_size)]
    return new

def flatten_data(data):
	flat_data = (data.reshape(data.shape[0] * data.shape[2], data.shape[1]))
	return flat_data

X_train = reshape(X_train)
X_test = reshape(X_test)
y_train = flatten_data(y_train)
y_test = flatten_data(y_test)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_train = torch.from_numpy(X_train)
le = preprocessing.LabelEncoder()
le.classes_ = np.load(directory+'encoder.npy')
# y_train = le.fit_transform(y_train)
y_train = torch.from_numpy(y_train)

X_test = torch.from_numpy(X_test)
test_label = le.transform(test_label)
y_test = torch.from_numpy(y_test)

criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=3e-4)

#create dataloaders for all 3 datasets

print(X_train.shape)
print(y_train.shape)

train_set = TensorDataset(X_train, y_train)
train_dataloader = DataLoader(train_set, batch_size = 126, num_workers = 2, shuffle=True)


test_set = TensorDataset(X_test, y_test)
test_dataloader = DataLoader(test_set, batch_size = 126, num_workers = 2)

count = 0
for epoch in range(50):
  print(epoch)
  running_loss = 0.0

  for i, data in enumerate(train_dataloader):
    #get input data and labels
    inputs, labels = data

    #labels need to be LongTensor type for some reason
    labels = labels.type(torch.FloatTensor)

    #inputs need to be floats
    inputs = inputs.float()

    #reshape inputs as (channels, batch size, 20, 126)
    #(20, 126) at end should be constant due to our preprocessing of the
    #MFCCs
    # inputs = torch.reshape(inputs, (1, inputs.shape[0], 16000))
    # inputs = torch.permute(inputs, (1, 0))

    #send data to gpus
    inputs = inputs.to(device)
    labels = labels.to(device)
    # print(inputs.size())
    # print(labels.size())
    #zero the gradients in optimizer
    optimizer.zero_grad()

    #create predictions, calculate loss, backward propogation, then
    #update weights of model
    outputs = net(inputs)
    # if(torch.count_nonzero(outputs) == 0):
    #   count += 1
    # loss = matrix_similarity_loss(outputs, labels)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    #add loss
    running_loss += float(loss.item())

    # total = 0
    # outputs = torch.reshape(outputs, (1, outputs.shape[0], outputs.shape[1], outputs.shape[2]))
    # final_outputs = conv_net(outputs)

    # #calculate class with highest probability
    # _, predicted = torch.max(final_outputs.data, 1)
    # y_pred = predicted
    # # y_pred.extend(predicted)

    # #update total and correct counts
    # total += labels.size(0)
    # correct += (predicted == labels).sum().item()

    # print(f'Accuracy of the network on the {total} validation images: {100 * correct // total} %')
    # print(f'Total count: {total}, correct: {correct}')

    #print update every 32 batches
    if i % 32 == 31:
      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 32:.3f}')
      running_loss = 0.0
print(count)


######Testing#######
correct = 0
total = 0 
test_loss = 0.0
y_pred = []
#use no_grad as we are testing
with torch.no_grad():

  #loop through test data
  for i, data in enumerate(test_dataloader):
    #gets inputs and labels
    inputs, labels = data 

    #convert labels to long tensors
    labels = labels.type(torch.FloatTensor)

    #convert inputs to float
    inputs = inputs.float()

    #reshape inputs as (channels, batch size, 16000)
    # inputs = torch.reshape(inputs, (1, inputs.shape[0], 16000))
    # inputs = torch.permute(inputs, (1, 0, 2, 3))

    #send data to gpus
    inputs = inputs.to(device)
    labels = labels.to(device)

    # print(inputs.shape)
    # print(labels.shape)
    #predict outputs
    outputs = net(inputs)

    outputs = torch.reshape(outputs, (1, 1, outputs.shape[0], outputs.shape[1]))
    # voutputs = torch.reshape(voutputs, (voutputs.shape[0], 1, voutputs.shape[1], voutputs.shape[2]))
    outputs = torch.permute(outputs, (0, 1, 3, 2))
      
    outputs_final = conv_net(outputs)
    # print(outputs_final.shape)
    #calculate class with highest probability
    predicted = np.array(torch.max(outputs_final.data, 1).indices)
    # _, predicted = torch.max(outputs.data, 1) 
    y_pred.extend(predicted)

    #update total and correct counts
    # total += labels.size(0)
    # loss = criterion(outputs, labels)
    # test_loss += loss.item()

    # test_label = torch.from_numpy(test_label)
    # test_label = test_label.to(device)
    # correct += (predicted == test_label).sum().item()

    if (i%32==0):
      print(i)

print(sum(y_pred==y_test))